{
 "cells": [
  {
   "cell_type": "code",
   "id": "206a1086",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scanpy as sc\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from mucstpy.utils import add_contrastive_label, get_feature\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['R_HOME'] = 'C:/Program Files/R/R-4.3.1'"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "34f4f629",
   "metadata": {},
   "source": [
    "adata = sc.read_h5ad('D:/st_projects/data/slide_seq/v2/hippocampus/slideseqv2.h5ad')\n",
    "adata.obsm['spatial'][:, 1] = -adata.obsm['spatial'][:, 1]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5aac64a5",
   "metadata": {},
   "source": [
    "sc.pp.calculate_qc_metrics(adata, inplace=True)\n",
    "adata = adata[:, adata.var['total_counts'] > 100]\n",
    "\n",
    "sc.pp.highly_variable_genes(adata, flavor=\"seurat_v3\", n_top_genes=3000)\n",
    "\n",
    "sc.pp.filter_genes(adata, min_cells=1)\n",
    "sc.pp.normalize_total(adata, target_sum=1e4)\n",
    "sc.pp.log1p(adata)\n",
    "sc.pp.scale(adata, zero_center=False, max_value=10)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "41815aca",
   "metadata": {},
   "source": [
    "sc.pl.embedding(adata, basis='spatial', color='cluster', size=10)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "bf8f0394",
   "metadata": {},
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "cluster_num = pd.get_dummies(adata.obs['cluster']).shape[1]\n",
    "cluster_num, device"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a92e323c",
   "metadata": {},
   "source": [
    "### For SRT data without histology image, just set $\\lambda_1$ =0"
   ]
  },
  {
   "cell_type": "code",
   "id": "32064069",
   "metadata": {},
   "source": [
    "from mucstpy.utils import construction_interaction\n",
    "\n",
    "construction_interaction(adata=adata, n_neighbor=15)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e2f347f7",
   "metadata": {},
   "source": [
    "add_contrastive_label(adata)\n",
    "get_feature(adata)\n",
    "\n",
    "gene_dims=[adata.shape[1], 64]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "92de274a",
   "metadata": {},
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from mucstpy.model import Encoder, Decoder, Discriminator, AvgReadout\n",
    "\n",
    "class MuCST_no_his(nn.Module):\n",
    "    def __init__(self, gene_dims, graph_nei):\n",
    "        super().__init__()\n",
    "        self.graph = graph_nei\n",
    "        # in_dim -> hidden -> project[0]\n",
    "        self.gene_encoder_layer1 = Encoder(in_dims=gene_dims[0], hidden_dims=gene_dims[1:])\n",
    "        self.decoder = Decoder(hidden_dims=list(reversed(gene_dims[1:])), out_dims=gene_dims[0])\n",
    "        self.disc = Discriminator(gene_dims[-1])\n",
    "        self.sigma = nn.Sigmoid()\n",
    "        self.read = AvgReadout()\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "\n",
    "    def forward_gene(self, gene, graph):\n",
    "        zg = self.gene_encoder_layer1.forward(x=gene, adj=graph)\n",
    "        return zg\n",
    "    \n",
    "    def recon_gene_loss(self, zg, xg, graph):\n",
    "        zg = self.decoder(zg, graph)\n",
    "        return self.mse_loss(zg, xg)\n",
    "    \n",
    "    def forward(self, gene, fake_gene , graph):\n",
    "        # encode the gene expression into latent embeddings\n",
    "        zg = self.forward_gene(gene, graph)\n",
    "        zg_fake = self.forward_gene(fake_gene, graph)\n",
    "\n",
    "        emb_true = F.relu(zg)\n",
    "        emb_fake = F.relu(zg_fake)\n",
    "        \n",
    "        g = self.read(emb_true, self.graph)\n",
    "        g = self.sigma(g)\n",
    "        g_fake = self.read(emb_fake, self.graph)\n",
    "        g_fake = self.sigma(g_fake)\n",
    "\n",
    "        dis_a = self.disc(g, emb_true, emb_fake)\n",
    "        dis_b = self.disc(g_fake, emb_fake, emb_true)\n",
    "        \n",
    "        rec_gene = self.decoder(zg, graph)\n",
    "        # rec_gene = F.relu(rec_gene)\n",
    "        return zg, rec_gene, dis_a, dis_b"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ddd1ff91",
   "metadata": {},
   "source": [
    "features = torch.FloatTensor(adata.obsm['feat'].copy()).to(device)\n",
    "features_fake = torch.FloatTensor(adata.obsm['feat_fake'].copy()).to(device)\n",
    "label_cont = torch.FloatTensor(adata.obsm['label_CSL']).to(device)\n",
    "adj = adata.obsm['adj']\n",
    "graph_neigh = torch.FloatTensor(adata.obsm['graph_neigh'].copy() + np.eye(adj.shape[0])).to(device)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9fd6693e",
   "metadata": {},
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "model = MuCST_no_his(gene_dims=gene_dims, graph_nei=graph_neigh).to(device)\n",
    "\n",
    "loss_CSL = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=0.)\n",
    "\n",
    "print('Begin to train MuCST without histology image...')\n",
    "model.train()\n",
    "\n",
    "for epoch in tqdm(range(1500)):\n",
    "    hidden_fea, rec_data, ret, ret_fake = model(features, features_fake, graph_neigh)\n",
    "    loss_cont = loss_CSL(ret, label_cont)\n",
    "    loss_cont_dual = loss_CSL(ret_fake, label_cont)\n",
    "    loss_feat = F.mse_loss(features, rec_data)\n",
    "\n",
    "    loss = loss_feat + 0.1 * (loss_cont + loss_cont_dual)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "07776090",
   "metadata": {},
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    _, rec_feature, _, _ = model.forward(features, features_fake, graph_neigh)\n",
    "    rec_feature = rec_feature.detach().cpu().numpy()\n",
    "    \n",
    "adata.obsm['rec_feature'] = rec_feature\n",
    "pca = PCA(n_components=50, random_state=2023)\n",
    "rec_feat_pca = pca.fit_transform(rec_feature)\n",
    "adata.obsm['rec_feat_pca'] = rec_feat_pca"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "sc.set_figure_params(figsize=(4, 4))\n",
    "sc.tl.leiden(adata, key_added='leiden_rec_feature', resolution=0.3)\n",
    "sc.pl.embedding(adata, basis='spatial', color=['leiden_rec_feature'], size=30)"
   ],
   "id": "c20939bb6516de42",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "735f6319873a4e4c",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mucst-jupyter",
   "language": "python",
   "name": "mucst"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
